# -*- coding: utf-8 -*-
"""WordNet.py

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1X24jGqZ4Mm8HkBTPTFfiS38PiAbwmZFq

## WordNet

WordNet is a large lexical database of the English language which consists of nouns, verbs, adjectives, and adverbs. These components are grouped into sets of cognitive synonyms which are called synsets which each represent a specific concept.
"""

import nltk
from nltk.corpus import wordnet as wn
from nltk.wsd import lesk

wn.synsets('word')

print(wn.synset('word.n.02').definition())
print(wn.synset('word.n.02').examples())
print(wn.synset('word.n.02').lemmas())

hyp = wn.synsets('word')[0]
top = wn.synset('entity.n.01')

while hyp:
  print(hyp)
  if hyp == top:
    break
  if hyp.hypernyms():
    hyp = hyp.hypernyms()[0]

"""Here you can see that the nouns are organized in a way in that they are grouped by similar concepts. Entity is closest to abstraction which is then related to abstraction. Also, you can see that the top is the closest to the word while as you go down it gets further and further away in concept from the original word."""

print(wn.synset('word.n.01').hypernyms())
print(wn.synset('word.n.01').hyponyms())
print(wn.synset('word.n.01').part_meronyms())
print(wn.synset('word.n.01').part_holonyms())

wn.synsets('assemble')

print(wn.synset('assemble.v.01').definition())
print(wn.synset('assemble.v.01').examples())
print(wn.synset('assemble.v.01').lemmas())

print(wn.synset('assemble.v.01').hypernyms())
print(wn.synset('assemble.v.01').hyponyms())
print(wn.synset('assemble.v.01').part_meronyms())
print(wn.synset('assemble.v.01').part_holonyms())

hyp = wn.synsets('assemble')[0]
top = wn.synset('assemble.v.01')

while hyp:
  print(hyp)
  if hyp == top:
    break
  if hyp.hypernyms():
    hyp = hyp.hypernyms()[0]

"""Here you can see that the verbs are also organized in a way in that they are grouped by similar concepts. You can see that the top verb is the closest to the word while as you go down it gets further and further away in concept from the original word."""

print(wn.morphy('running', wn.NOUN))
print(wn.morphy('running', wn.VERB))
print(wn.morphy('running', wn.ADJ))
print(wn.morphy('running', wn.ADV))

pet = wn.synset('pet.n.01')
animal = wn.synset('animal.n.01')
print(pet.path_similarity(animal))

print(wn.wup_similarity(pet,animal))

sent = ['I', 'think', 'I' , 'want', 'a', 'pet', 'that', 'will', 'listen', 'to', 'me', '.']
print(lesk(sent, 'pet', 'n'))

sent = ['I', 'think', 'I' , 'want', 'an', 'animal', 'that', 'will', 'listen', 'to', 'me', '.']
print(lesk(sent, 'animal', 'n'))

"""What we can see is that the words pet and animal are actually not that similar in definition but have common ancestor words as seen according to the Wu-Palmer similarity metrics. Additionally, it seems that the Lesk algorithim is not always effective due to the fact that giving it the word 'pet' confuses it and thinks that it is a positron emission tomography whihc is not actually the case.

##SentiWordNet

SentiWordNet is a lexical resource which works with WordNet and assigns 3 various sentiment scores for each synset. These 3 sentiments that are described as positivity, negativity, and objectivity and are scored by this tool
"""

from nltk.corpus import sentiwordnet as swn

breakdown = swn.senti_synset('breakdown.n.02')
print("Positive score = ", breakdown.pos_score())
print("Negative score = ", breakdown.neg_score())
print("Objective score = ", breakdown.obj_score())

sent = 'i hate that you felt that you needed to do that'
neg = 0
pos = 0
tokens = sent.split()

for token in tokens:
  syn_list = list(swn.senti_synsets(token))
  if syn_list:
    syn = syn_list[0]
    neg += syn.neg_score()
    pos += syn.pos_score()

print("neg\tpos counts")
print(neg, '\t', pos)

"""As you can see this feature in which the sentiment analysis of a sentence can be done has many applications. By being able to analyze the sentiment, you can understand the tone of the setence and make inferences.

## Collocation

A collocation is a interesting way to look at words and how they interact when grouped together. The term refers to the situation where a group of two or more words are grouped together. A collocation is clearly identified when there is no clear subsitute synonym.
"""

from nltk.book import *

text4.collocations()

text = ' '.join(text4.tokens)
text

import math
vocab = len(set(text4))
hg = text.count('United States')/vocab
print("p(United States)) = ",hg )
h = text.count('United')/vocab
print("p(United) = ", h)
g = text.count('States')/vocab
print('p(States) = ', g)
pmi = math.log2(hg / (h * g))
print('pmi = ', pmi)

vocab = len(set(text4))
hg = text.count('fellow citizens')/vocab
print("p(fellow citizens)) = ",hg )
h = text.count('fellow')/vocab
print("p(fellow) = ", h)
g = text.count('citizens')/vocab
print('p(citizens) = ', g)
pmi = math.log2(hg / (h * g))
print('pmi = ', pmi)

"""What we can tell after calculating the mutual information for the collocations, "United States" and "fellow citizens" is that the collocation with the higher mutual information is more likely to occur. As a result, "United States" evidently occurs more within text4"""